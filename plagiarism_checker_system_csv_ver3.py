# -*- coding: utf-8 -*-
"""Plagiarism_Checker_System_csv_ver3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fc_42yinUuOMcY6wgFsmYErmD55PtLIk
"""

# Import all necessary library and function
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import requests
import pandas as pd
import io
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
nltk.download('punkt')
from tensorflow.keras.preprocessing.text import Tokenizer
import warnings
warnings.filterwarnings('ignore')

# Defining the function to remove unnecesary symbols and empty arrays
def remove(text):
  pattern = r'[“”‘’:;"_\',.()\–\[\]]'
  sub_text = re.sub(pattern,'', text)
  pattern = r'[\-]'
  sub_text = re.sub(pattern,' ', sub_text)
  token_text = word_tokenize(sub_text)
  words = [word for word in token_text if word]
  return ' '.join(words)

# Cleaning the text by iterating through the line
def get_list(data):
  list_arr = []
  text_to_line = sent_tokenize(data.lower())
  for text in text_to_line:
    clean_line = remove(text)
    list_arr.append(clean_line)
  return list_arr

# Creating the text array of all files
def text_list(story_data):
  lists = []
  for _, row in story_data.iterrows():
    text_arr = get_list(row[-2])
    lists.append(text_arr)
  return lists

# Defining the Token of each text
def tokenizing(flat_text,text):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(flat_text)
  word_index = tokenizer.word_index
  tokenid = []
  for i in text:
    tokens = tokenizer.texts_to_sequences(i)
    tokenid.append(tokens)
  return tokenid

# Defining the TfidfVectorizer of each text
def vecTfid(flat_text,text):
  vec = TfidfVectorizer()
  vec.fit(flat_text)
  vecarr = []
  for i in text:
    transform = vec.transform(i).toarray()
    vecarr.append(transform)
  return vecarr

# To make all the line in file author and similar lines in other files into a dictionary
def add_values(story_dict, key, values, id):
  if key not in story_dict:
    story_dict[key] = {}
  if id not in story_dict[key]:
    story_dict[key][id] = []
  for value in values:
    if value not in story_dict[key][id]:
      story_dict[key][id].append(value)

# Using the flag system to count copy-pasted and high similarity line

def count_flag(token1, token2, tf1, tf2, story_dict, text1, text2, title):
  # Defining all necessary flag variables
  plag_tf_token = 0
  di = len(tf1)
  token_cos = []
  cp = 0
  hs = 0

  # Getting the similarity on Tokenized text and Vectorized text
  tf_cos = cosine_similarity(tf1,tf2)
  for i in range(len(token1)):
    cos_line= []
    for j in range(len(token2)):
      leng = len(list(set(token1[i]+token2[j])))
      cos = len(set(token1[i])&set(token2[j]))/leng
      cos_line.append(cos)
    token_cos.append(cos_line)

  # Both Tokenized and Vectorized has their own strength and uses
  # Similarity threshold (0.9999, 0.7, and 0.35) may be changed
  # Flag system works by:
  # 1. Flag each line that are copy-pasted or high similarity with value 1, final result is the total line
  # 2. Line with less than 0.9999 similarity uses several threshold (0.7 and 0.3)
       # Both threshold gave different flag value (for tf) (plag_tf_token)
       # Final result of plag_tf_token is the summation of flag tf and token if total > 1
  for i in range(di):
    tf = 0
    token = 0
    # For high similarity checking
    if any(vals >= 0.9999 for vals in tf_cos[i]):
      tryis = [index for index, vals in enumerate(tf_cos[i]) if vals >= 0.9999]
      add_values(story_dict, text1[i],[text2[i] for i in tryis], f"file {title}")
      hs+=1
    elif any(0.7 < vals < 0.9999 for vals in tf_cos[i]):
      tryis = [index for index, vals in enumerate(tf_cos[i]) if 0.7 < vals < 0.9999]
      add_values(story_dict, text1[i],[text2[i] for i in tryis], f"file {title}")
      tf = 2
    elif any(0.35 < vals <= 0.7 for vals in tf_cos[i]):
      tryis = [index for index, vals in enumerate(tf_cos[i]) if 0.35 < vals <= 0.7]
      add_values(story_dict, text1[i],[text2[i] for i in tryis], f"file {title}")
      tf = 1
    else:
      tf = 0

    # For high structure similarity checking
    if any(vals >= 0.9999 for vals in token_cos[i]):
      tryis = [index for index, vals in enumerate(token_cos[i]) if vals >= 0.9999]
      add_values(story_dict, text1[i],[text2[i] for i in tryis], f"file {title}")
      cp+=1
    elif any(0.7 < vals < 0.9999 for vals in token_cos[i]):
      tryis = [index for index, vals in enumerate(token_cos[i]) if 0.7 < vals < 0.9999]
      add_values(story_dict, text1[i],[text2[i] for i in tryis], f"file {title}")
      token = 1
    else:
      token = 0

    # For plagiarism checking score (other than copy-pasted or 99% similarity)
    if token + tf > 1:
      plag_tf_token += 1
  return plag_tf_token, di, hs, cp

# Main code of the system, calling all the neccesary functions

def main_code(story_data,text_list):
  # Flattened the text to smooth out the tokenize and vectorize
  flat_text = [line for text in text_list for line in text]

  # Calling the tokenizer and vectorizer function
  tfid = vecTfid(flat_text,text_list)
  token = tokenizing(flat_text,text_list)

  fin_plag_score = 0
  story_dict = {}

  # Iterating each text to examine the similarity between text
  for j in range(len(token)):
    # Skip chapter text of same story
    if story_data.iloc[-1,1] != story_data.iloc[j,1]:
      plag_ft, di, hs, cp = count_flag(token[-1], token[j], tfid[-1], tfid[j], story_dict, text_list[-1], text_list[j], story_data.iloc[j,1])
      plag_score = (plag_ft+(hs+cp)/2)/di
    if plag_score > fin_plag_score:
      fin_plag_score = plag_score
  final = pd.DataFrame({'Final Score': [fin_plag_score*100]})
  data = []
  for key, file_dict in story_dict.items():
    for id, value in file_dict.items():
      row = {'Key': key, 'File': id, 'Value': value}
      data.append(row)
  df = pd.DataFrame(data)
  df_e = df.explode('Value')
  df_e.reset_index(drop=True, inplace=True)
  return df.to_json(), final.to_json(), df_e.to_json()

def Plagiarism_Checker(story_data):
  text = text_list(story_data)
  show_arr = main_code(story_data,text)
  return show_arr

def request(story_url='https://docs.google.com/spreadsheets/d/e/2PACX-1vQxre--PXKwKBMdhZGv7XmpUPkBAiXg84lETpmkftFNusZ2MLZOpq6jb4MPk3TQ02T-FBcO17Ui4X7l/pub?gid=1217383481&single=true&output=csv'):
  story = requests.get(story_url)
  story = story.content
  story_data = pd.read_csv(io.StringIO(story.decode('utf-8')))
  return story_data

df, final, df_e = Plagiarism_Checker(request())

print(df)

#df_e yg masing2 similar line jadi beda row&index
#df yg semua similar line jadi 1d di index yg sama
#final hasil akhir plagiarism score (ngambil nilai score plagiarism tertinggi)
