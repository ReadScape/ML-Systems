{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Necessary Library**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jm1AlW8ISs2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lWjJMunMSlEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6c6c6f-1c6f-4cc4-da52-711cbe1d76c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary library and function\n",
        "\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "!pip install python-docx\n",
        "import docx\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outline:**"
      ],
      "metadata": {
        "id": "h_7_qYa-bLtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "1.   Similarity Check Function    | 2.   PDF                               | 3.   Word\n",
        "     - TfidfVectorizer            |     - PDF Path                         |     - Word Path\n",
        "     - Tokenizer                  |     - Extract PDF                      |     - Extract Word\n",
        "     - Cosine_similarity          |     - Removing and Tokenizing Text     |     - Removing and Tokenizing Text\n",
        "     - Padding text               |     - Creating Text List               |     - Creating Text List\n",
        "                                  |     - PDF Plagiarism Check Function    |     - Word Plagiarism Check Function\n",
        "                                  |     - Example                          |     - Example\n",
        "```"
      ],
      "metadata": {
        "id": "fKMWg4--gfVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenize, Vectorize, and Padding**"
      ],
      "metadata": {
        "id": "82T5d2AyVQkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Token of each text\n",
        "\n",
        "def tokenizing(flat_text,text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(flat_text)\n",
        "  word_index = tokenizer.word_index\n",
        "  tokenid = []\n",
        "  for i in text:\n",
        "    tokens = tokenizer.texts_to_sequences(i)\n",
        "    tokenid.append(tokens)\n",
        "  return tokenid"
      ],
      "metadata": {
        "id": "vR2kRHe6QXzM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the TfidfVectorizer of each text\n",
        "\n",
        "def vecTfid(flat_text,text):\n",
        "  vec = TfidfVectorizer()\n",
        "  vec.fit(flat_text)\n",
        "  vecarr = []\n",
        "  for i in text:\n",
        "    transform = vec.transform(i).toarray()\n",
        "    vecarr.append(transform)\n",
        "  return vecarr"
      ],
      "metadata": {
        "id": "U1pQhvm2QUgC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Pad Sequences on the Tokenized text\n",
        "\n",
        "def padding(maxlen,sequences):\n",
        "  padded_seq = []\n",
        "  for i in range(len(sequences)):\n",
        "    pad = pad_sequences(sequences[i], maxlen=maxlen, padding='post')\n",
        "    padded_seq.append(pad)\n",
        "  return padded_seq"
      ],
      "metadata": {
        "id": "umZgQgoYQYWi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Similarity Check**"
      ],
      "metadata": {
        "id": "LURrgcllwGhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the flag system to count copy-pasted and high similarity line\n",
        "\n",
        "def count_flag(pad1, pad2, tf1, tf2):\n",
        "  # Getting the similarity on Tokenized text and Vectorized text\n",
        "  tf_cos = cosine_similarity(tf1,tf2)\n",
        "  token_cos = cosine_similarity(pad1,pad2)\n",
        "\n",
        "  # Defining all necessary flag variables\n",
        "  red_flag_tf = 0\n",
        "  red_flag_token = 0\n",
        "  tf = 0\n",
        "  token = 0\n",
        "  plag_tf_token = 0\n",
        "  di = len(tf_cos)\n",
        "\n",
        "  # Both Tokenized and Vectorized has their own strength and uses\n",
        "  # Similarity threshold (0.999, 0.7, and 0.3) may be changed\n",
        "  # Flag system works by:\n",
        "  # 1. Flag each line that are copy-pasted or high similarity with value 1, final result is the total line\n",
        "  # 2. Line with less than 0.9999 similarity uses several threshold (0.7 and 0.3)\n",
        "       # Both threshold gave different flag value (for tf) (plag_tf_token)\n",
        "       # Final result of plag_tf_token is the summation of flag tf and token if total > 1\n",
        "  for i in range(di):\n",
        "    if any(vals > 0.99 for vals in tf_cos[i]):\n",
        "      red_flag_tf += 1\n",
        "    if any(vals > 0.99 for vals in token_cos[i]):\n",
        "      red_flag_token += 1\n",
        "  for i in range(di):\n",
        "    if any(vals >= 0.7 for vals in tf_cos[i]):\n",
        "      tf = 2\n",
        "    elif any(vals >= 0.3 for vals in tf_cos[i]):\n",
        "      tf = 1\n",
        "    else:\n",
        "      tf = 0\n",
        "    if any(vals >= 0.7 for vals in token_cos[i]):\n",
        "      token = 1\n",
        "    else:\n",
        "      token = 0\n",
        "    if token + tf > 1:\n",
        "      plag_tf_token += 1\n",
        "  return red_flag_tf, red_flag_token, plag_tf_token, di"
      ],
      "metadata": {
        "id": "7MkLbZBtQYc9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remove Unnecessary Symbols**"
      ],
      "metadata": {
        "id": "mzvJtYBuYBEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to remove unnecesary symbols and empty arrays\n",
        "\n",
        "def remove(text):\n",
        "  pattern = r'[“”‘’:;\"_\\',.()\\–\\[\\]\\-]'\n",
        "  sub_text = re.sub(pattern,'', text)\n",
        "  token_text = word_tokenize(sub_text)\n",
        "  words = [word for word in token_text if word]\n",
        "  return ' '.join(words)"
      ],
      "metadata": {
        "id": "8bwj38lMXxH2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word**"
      ],
      "metadata": {
        "id": "763qMk5_DGVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Path**"
      ],
      "metadata": {
        "id": "TgzNwyE6ajgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining all text files that endswith .docx\n",
        "\n",
        "word_path = '/content/try-1'\n",
        "\n",
        "def word_file(word_path):\n",
        "  word_file_list = np.array([file for file in os.listdir(word_path) if file.lower().endswith('.docx')],dtype=object)\n",
        "  return word_file_list"
      ],
      "metadata": {
        "id": "swQ8ZAy-DGqU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Word**"
      ],
      "metadata": {
        "id": "8XriqvrIaSO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the text from word files\n",
        "\n",
        "def get_word_data(word_path,word_file):\n",
        "  word_read = docx.Document(word_path+'/'+word_file)\n",
        "\n",
        "  # Defining the necessary asrrays and variables\n",
        "  par_arr = []\n",
        "  split_arr = []\n",
        "  lindex = 0\n",
        "  refindex = -1\n",
        "\n",
        "  #Reading each paragraphs inside the text\n",
        "  for par in word_read.paragraphs:\n",
        "    sentences = sent_tokenize(par.text.lower())\n",
        "    for sent in sentences:\n",
        "      lines = sent.split('\\n')\n",
        "      lines = [remove(line) for line in lines]\n",
        "      lines = [line for line in lines if line]\n",
        "      par_arr.extend(lines)\n",
        "\n",
        "  # To define the first index and last index, may be removed depending on the format of text\n",
        "  if 'latar belakang' in par_arr:\n",
        "    lindex = par_arr.index('latar belakang')\n",
        "    lindex+=1\n",
        "  if 'daftar pustaka' in par_arr:\n",
        "    refindex = par_arr.index('daftar pustaka')\n",
        "  elif 'reference' in par_arr:\n",
        "    refindex = par_arr.index('reference')\n",
        "  if lindex > 0 or refindex > 0:\n",
        "    return par_arr[lindex:refindex]\n",
        "  else:\n",
        "    return par_arr"
      ],
      "metadata": {
        "id": "O1yUgKAqIRiP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get Word Text List**"
      ],
      "metadata": {
        "id": "pb1OqdYlashT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an array containing the file name and text of each files\n",
        "\n",
        "def list_word_text(word_path,word_file):\n",
        "  text_list = np.empty((0, 2), dtype=str)\n",
        "  for files in word_file:\n",
        "    text = get_word_data(word_path,files)\n",
        "    text_list = np.append(text_list,[[files,text]],axis=0)\n",
        "  return text_list"
      ],
      "metadata": {
        "id": "KloV6ZkiK8bN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF**"
      ],
      "metadata": {
        "id": "huvob2oseq1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDF Path**"
      ],
      "metadata": {
        "id": "vUJxoiiFXwib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining all text files that endswith .pdf\n",
        "\n",
        "folder_pdf = '/content/tryp'\n",
        "\n",
        "def pdf_file(folder_pdf):\n",
        "  pdf_file_list = np.array([file for file in os.listdir(folder_pdf) if file.lower().endswith('.pdf')],dtype=object)\n",
        "  return pdf_file_list"
      ],
      "metadata": {
        "id": "J4W234TfUoba"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract PDF**\n",
        "---\n"
      ],
      "metadata": {
        "id": "i8BklE2Qvqc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the text from pdf files\n",
        "\n",
        "def get_pdf(path,file_name):\n",
        "  # Defining the necessary array and variables\n",
        "  lindex = 0\n",
        "  refindex = -1\n",
        "  line_arr = []\n",
        "\n",
        "  # Reading text in pdf file by page\n",
        "  with open(path+'/'+file_name,'rb') as temp_pdf:\n",
        "    read_pdf = PyPDF2.PdfReader(temp_pdf)\n",
        "    for num_page in range(len(read_pdf.pages)):\n",
        "      page = read_pdf.pages[num_page].extract_text().lower()\n",
        "      sentence = sent_tokenize(page)\n",
        "      for sent in sentence:\n",
        "        lines = sent.split('\\n')\n",
        "        lines = [remove(line) for line in lines]\n",
        "        lines = [line for line in lines if line]\n",
        "        line_arr.extend(lines)\n",
        "\n",
        "  # To define the first index and last index, may be removed depending on the format of text\n",
        "  if 'latar belakang' in line_arr:\n",
        "    lindex = line_arr.index('latar belakang')\n",
        "    lindex+=1\n",
        "  if 'daftar pustaka' in line_arr:\n",
        "    refindex = line_arr.index('daftar pustaka')\n",
        "  elif 'reference' in line_arr:\n",
        "    refindex = line_arr.index('reference')\n",
        "  if lindex > 0 or refindex > 0:\n",
        "    return line_arr[lindex:refindex]\n",
        "  else:\n",
        "    return line_arr"
      ],
      "metadata": {
        "id": "VkA-9QWCg7Fx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get PDF Text List**"
      ],
      "metadata": {
        "id": "08BZdxYrYM-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an array containing the file name and text of each files\n",
        "\n",
        "def list_all_text(pdf_folder_path,pdf_file_list):\n",
        "  text_list = np.empty((0, 2), dtype=str)\n",
        "  for files in pdf_file_list:\n",
        "    text = get_pdf(pdf_folder_path,files)\n",
        "    text_list = np.append(text_list,[[files,text]],axis=0)\n",
        "  return text_list"
      ],
      "metadata": {
        "id": "AlNaliB5m49I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Code**"
      ],
      "metadata": {
        "id": "yNvGaj4TUNaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main code of the system, calling all the neccesary functions\n",
        "\n",
        "def main_code(text_list):\n",
        "  # Separating the filenames and texts from text_list array\n",
        "  name = [text[0] for text in text_list]\n",
        "  text = [text[1] for text in text_list]\n",
        "\n",
        "  # Flattened the text to smooth out the tokenize and vectorize\n",
        "  flat_text = [line for text in text for line in text]\n",
        "\n",
        "  # Calling the tokenizer and vectorizer function\n",
        "  tfid = vecTfid(flat_text,text)\n",
        "  token = tokenizing(flat_text,text)\n",
        "\n",
        "  # Getting the maximum num of line among all text and use it for padding\n",
        "  maxlen_token = max([len(word) for line in token for word in line])\n",
        "  pad_token = padding(maxlen_token,token)\n",
        "\n",
        "  # Iterating each text to examine the similarity between text\n",
        "  for i in range(len(pad_token)):\n",
        "    for j in range(i+1,len(pad_token)):\n",
        "      flag_tf, flag_token, plag_tf_token, di = count_flag(pad_token[i], pad_token[j], tfid[i], tfid[j])\n",
        "      plag_score = (plag_tf_token+flag_tf+flag_token)/(di*3)\n",
        "\n",
        "      # Plagiarized score are categorized into 2 section\n",
        "      # Those with plag_score more than 25% and those with less than 25%\n",
        "      # For plag score with more than 25%, a warning will be displayed, along with the number of copy-pasted and high similarity line\n",
        "      # For plag score with less than 25%, however, has copy-pasted and high similarity line, a warning will be displayed\n",
        "      # For plag score that has less than 25% and no copy-pasted and high similarity line, no warning will be displayed\n",
        "      print(f\"file {name[i]} vs file {name[j]}\")\n",
        "      if plag_score >= 0.3:\n",
        "        print(f\"\\t!Warning! There are overall {plag_score*100:.2f}% similarity score in both file! Bigger than 30%!\")\n",
        "        if flag_tf > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {flag_tf} line with 99% similarity! About {flag_tf/di*100:.2f}% of both text!\")\n",
        "        if flag_token > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {flag_token} line with 99% similar structure! About {flag_token/di*100:.2f}% of both text!\")\n",
        "      elif plag_score < 0.3 :\n",
        "        if flag_tf > 0:\n",
        "          print(f\"\\t!Warning! There are {flag_tf} line with 99% similarity in both file! About {flag_tf/di*100:.2f}% of both text!\")\n",
        "        if flag_token > 0:\n",
        "          print(f\"\\t!Warning! There are {flag_token} line with 99% similar structure in both file! About {flag_token/di*100:.2f}% of both text!\")\n",
        "        else:\n",
        "          print(f\"\\tSimilarity score is {plag_score*100:.2f}%! Congratulations, you may upload your work :)!\")"
      ],
      "metadata": {
        "id": "4NhtsOYHAFqg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Plagiarism Checker Function**"
      ],
      "metadata": {
        "id": "opQwdwI0a5I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Plagiarism Checker function for word files\n",
        "\n",
        "def Plagiarism_checker_word(word_path):\n",
        "  text_list = list_word_text(word_path,word_file(word_path))\n",
        "  main_code(text_list)"
      ],
      "metadata": {
        "id": "oT7VZxqP0lQl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word file Example**"
      ],
      "metadata": {
        "id": "okifPZBxTrM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Plagiarism_checker_word(word_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe5yyIhzhOnW",
        "outputId": "4f38ea64-0074-42ee-ec51-9834b01007a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file ann.docx vs file Try3.docx\n",
            "\t!Warning! There are overall 53.50% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 50 line with 99% similarity! About 42.02% of both text!\n",
            "\t\tAmong which, there are 54 line with 99% similar structure! About 45.38% of both text!\n",
            "file ann.docx vs file wid.docx\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "file ann.docx vs file has.docx\n",
            "\t!Warning! There are overall 38.66% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 35 line with 99% similarity! About 29.41% of both text!\n",
            "\t\tAmong which, there are 43 line with 99% similar structure! About 36.13% of both text!\n",
            "file ann.docx vs file nav.docx\n",
            "\t!Warning! There are overall 31.65% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 21 line with 99% similarity! About 17.65% of both text!\n",
            "\t\tAmong which, there are 31 line with 99% similar structure! About 26.05% of both text!\n",
            "file ann.docx vs file sab.docx\n",
            "\t!Warning! There are 6 line with 99% similarity in both file! About 5.04% of both text!\n",
            "\t!Warning! There are 13 line with 99% similar structure in both file! About 10.92% of both text!\n",
            "file Try3.docx vs file wid.docx\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "file Try3.docx vs file has.docx\n",
            "\t!Warning! There are overall 44.80% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 45 line with 99% similarity! About 36.00% of both text!\n",
            "\t\tAmong which, there are 47 line with 99% similar structure! About 37.60% of both text!\n",
            "file Try3.docx vs file nav.docx\n",
            "\t!Warning! There are 15 line with 99% similarity in both file! About 12.00% of both text!\n",
            "\t!Warning! There are 23 line with 99% similar structure in both file! About 18.40% of both text!\n",
            "file Try3.docx vs file sab.docx\n",
            "\t!Warning! There are 6 line with 99% similarity in both file! About 4.80% of both text!\n",
            "\t!Warning! There are 11 line with 99% similar structure in both file! About 8.80% of both text!\n",
            "file wid.docx vs file has.docx\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "file wid.docx vs file nav.docx\n",
            "\t!Warning! There are 1 line with 99% similar structure in both file! About 5.56% of both text!\n",
            "file wid.docx vs file sab.docx\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "file has.docx vs file nav.docx\n",
            "\t!Warning! There are 23 line with 99% similarity in both file! About 14.56% of both text!\n",
            "\t!Warning! There are 30 line with 99% similar structure in both file! About 18.99% of both text!\n",
            "file has.docx vs file sab.docx\n",
            "\t!Warning! There are 3 line with 99% similarity in both file! About 1.90% of both text!\n",
            "\t!Warning! There are 16 line with 99% similar structure in both file! About 10.13% of both text!\n",
            "file nav.docx vs file sab.docx\n",
            "\t!Warning! There are 4 line with 99% similarity in both file! About 2.14% of both text!\n",
            "\t!Warning! There are 20 line with 99% similar structure in both file! About 10.70% of both text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF Plagiarism Check Function**"
      ],
      "metadata": {
        "id": "i9rLKB6iYWso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Plagiarism Checker function for pdf files\n",
        "\n",
        "def Plagiarism_checker_pdf(pdf_path):\n",
        "  text_list = list_all_text(pdf_path,pdf_file(pdf_path))\n",
        "  main_code(text_list)"
      ],
      "metadata": {
        "id": "TyvwPC5xeaU5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDF file Example**"
      ],
      "metadata": {
        "id": "tdGhzK-8Tz4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Plagiarism_checker_pdf(folder_pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf5SsuOdfYkt",
        "outputId": "dde4d32c-c12f-42bf-be36-5a122400c328"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file wid.pdf vs file sab.pdf\n",
            "\t!Warning! There are 12 line with 99% similar structure in both file! About 27.91% of both text!\n",
            "file wid.pdf vs file rang.pdf\n",
            "\t!Warning! There are 12 line with 99% similar structure in both file! About 27.91% of both text!\n",
            "file wid.pdf vs file vir.pdf\n",
            "\t!Warning! There are 9 line with 99% similar structure in both file! About 20.93% of both text!\n",
            "file wid.pdf vs file zin.pdf\n",
            "\t!Warning! There are 8 line with 99% similar structure in both file! About 18.60% of both text!\n",
            "file wid.pdf vs file zo.pdf\n",
            "\t!Warning! There are 10 line with 99% similar structure in both file! About 23.26% of both text!\n",
            "file wid.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 10 line with 99% similar structure in both file! About 23.26% of both text!\n",
            "file sab.pdf vs file rang.pdf\n",
            "\t!Warning! There are overall 40.03% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 20 line with 99% similarity! About 4.67% of both text!\n",
            "\t\tAmong which, there are 233 line with 99% similar structure! About 54.44% of both text!\n",
            "file sab.pdf vs file vir.pdf\n",
            "\t!Warning! There are 146 line with 99% similar structure in both file! About 34.11% of both text!\n",
            "file sab.pdf vs file zin.pdf\n",
            "\t!Warning! There are 127 line with 99% similar structure in both file! About 29.67% of both text!\n",
            "file sab.pdf vs file zo.pdf\n",
            "\t!Warning! There are 160 line with 99% similar structure in both file! About 37.38% of both text!\n",
            "file sab.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 139 line with 99% similar structure in both file! About 32.48% of both text!\n",
            "file rang.pdf vs file vir.pdf\n",
            "\t!Warning! There are 148 line with 99% similar structure in both file! About 25.52% of both text!\n",
            "file rang.pdf vs file zin.pdf\n",
            "\t!Warning! There are 119 line with 99% similar structure in both file! About 20.52% of both text!\n",
            "file rang.pdf vs file zo.pdf\n",
            "\t!Warning! There are 158 line with 99% similar structure in both file! About 27.24% of both text!\n",
            "file rang.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 129 line with 99% similar structure in both file! About 22.24% of both text!\n",
            "file vir.pdf vs file zin.pdf\n",
            "\t!Warning! There are 6 line with 99% similar structure in both file! About 10.53% of both text!\n",
            "file vir.pdf vs file zo.pdf\n",
            "\t!Warning! There are 10 line with 99% similar structure in both file! About 17.54% of both text!\n",
            "file vir.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 6 line with 99% similar structure in both file! About 10.53% of both text!\n",
            "file zin.pdf vs file zo.pdf\n",
            "\t!Warning! There are 8 line with 99% similar structure in both file! About 15.09% of both text!\n",
            "file zin.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 9 line with 99% similar structure in both file! About 16.98% of both text!\n",
            "file zo.pdf vs file syaf.pdf\n",
            "\t!Warning! There are 10 line with 99% similar structure in both file! About 11.76% of both text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unused**"
      ],
      "metadata": {
        "id": "ZdJrklp4QNSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#  indstop = set(stopwords.words('indonesian'))\n",
        "#  indstop = list(indstop)\n",
        "#  engstop = set(stopwords.words('english'))\n",
        "#  engstop = list(engstop)\n",
        "#  remove_word = (indstop,engstop)\n",
        "#  remove_word = [word for line in remove_word for word in line]"
      ],
      "metadata": {
        "id": "U0n5OTMe4Va1"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}