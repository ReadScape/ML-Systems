{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lWjJMunMSlEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39552fdc-0814-413b-890a-2bb3b107ca42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary library and function\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def request(story_url='https://docs.google.com/spreadsheets/d/e/2PACX-1vQxre--PXKwKBMdhZGv7XmpUPkBAiXg84lETpmkftFNusZ2MLZOpq6jb4MPk3TQ02T-FBcO17Ui4X7l/pub?gid=1217383481&single=true&output=csv'):\n",
        "  story = requests.get(story_url)\n",
        "  story = story.content\n",
        "  story_data = pd.read_csv(io.StringIO(story.decode('utf-8')))\n",
        "  return story_data"
      ],
      "metadata": {
        "id": "jO87oakFNmSt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to remove unnecesary symbols and empty arrays\n",
        "\n",
        "def remove(text):\n",
        "  pattern = r'[“”‘’:;\"_\\',.()\\–\\[\\]]'\n",
        "  sub_text = re.sub(pattern,'', text)\n",
        "  pattern = r'[\\-]'\n",
        "  sub_text = re.sub(pattern,' ', sub_text)\n",
        "  token_text = word_tokenize(sub_text)\n",
        "  words = [word for word in token_text if word]\n",
        "  return ' '.join(words)"
      ],
      "metadata": {
        "id": "8bwj38lMXxH2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the text by iterating through the line\n",
        "\n",
        "def get_csv_list(data):\n",
        "  csv_arr = []\n",
        "  text_to_line = sent_tokenize(data.lower())\n",
        "  for text in text_to_line:\n",
        "    clean_line = remove(text)\n",
        "    csv_arr.append(clean_line)\n",
        "  return csv_arr"
      ],
      "metadata": {
        "id": "ChBGxFqaShoF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the text array of all files\n",
        "\n",
        "def text_list(story_data):\n",
        "  csv_list = []\n",
        "  for _, row in story_data.iterrows():\n",
        "    text_arr = get_csv_list(row[-2])\n",
        "    csv_list.append(text_arr)\n",
        "  return csv_list"
      ],
      "metadata": {
        "id": "SsfV0i0Gsirm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Token of each text\n",
        "\n",
        "def tokenizing(flat_text,text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(flat_text)\n",
        "  word_index = tokenizer.word_index\n",
        "  tokenid = []\n",
        "  for i in text:\n",
        "    tokens = tokenizer.texts_to_sequences(i)\n",
        "    tokenid.append(tokens)\n",
        "  return tokenid"
      ],
      "metadata": {
        "id": "vR2kRHe6QXzM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the TfidfVectorizer of each text\n",
        "\n",
        "def vecTfid(flat_text,text):\n",
        "  vec = TfidfVectorizer()\n",
        "  vec.fit(flat_text)\n",
        "  vecarr = []\n",
        "  for i in text:\n",
        "    transform = vec.transform(i).toarray()\n",
        "    vecarr.append(transform)\n",
        "  return vecarr"
      ],
      "metadata": {
        "id": "U1pQhvm2QUgC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the flag system to count copy-pasted and high similarity line\n",
        "\n",
        "def count_flag(token1, token2, tf1, tf2):\n",
        "  # Defining all necessary flag variables\n",
        "  plag_tf_token = 0\n",
        "  di = len(tf1)\n",
        "  high_sim = []\n",
        "  copy_paste = []\n",
        "  struc_token = []\n",
        "  struc_tf = []\n",
        "  token_cos = []\n",
        "\n",
        "  # Getting the similarity on Tokenized text and Vectorized text\n",
        "  tf_cos = cosine_similarity(tf1,tf2)\n",
        "  for i in range(len(token1)):\n",
        "    cos_line= []\n",
        "    for j in range(len(token2)):\n",
        "      leng = len(list(set(token1[i]+token2[j])))\n",
        "      cos = len(set(token1[i])&set(token2[j]))/leng\n",
        "      cos_line.append(cos)\n",
        "    token_cos.append(cos_line)\n",
        "\n",
        "  # Both Tokenized and Vectorized has their own strength and uses\n",
        "  # Similarity threshold (0.9999, 0.7, and 0.35) may be changed\n",
        "  # Flag system works by:\n",
        "  # 1. Flag each line that are copy-pasted or high similarity with value 1, final result is the total line\n",
        "  # 2. Line with less than 0.9999 similarity uses several threshold (0.7 and 0.3)\n",
        "       # Both threshold gave different flag value (for tf) (plag_tf_token)\n",
        "       # Final result of plag_tf_token is the summation of flag tf and token if total > 1\n",
        "  for i in range(di):\n",
        "    tf = 0\n",
        "    token = 0\n",
        "    # For high similarity checking\n",
        "    if any(vals >= 0.9999 for vals in tf_cos[i]):\n",
        "      high_sim.append([i,[index for index, vals in enumerate(tf_cos[i]) if vals >= 0.9999]])\n",
        "    elif any(0.7 < vals < 0.9999 for vals in tf_cos[i]):\n",
        "      struc_tf.append([i,[index for index, vals in enumerate(tf_cos[i]) if 0.7 < vals < 0.9999]])\n",
        "      tf = 2\n",
        "    elif any(0.35 < vals <= 0.7 for vals in tf_cos[i]):\n",
        "      struc_tf.append([i,[index for index, vals in enumerate(tf_cos[i]) if 0.35 < vals <= 0.7]])\n",
        "      tf = 1\n",
        "    else:\n",
        "      tf = 0\n",
        "\n",
        "    # For high structure similarity checking\n",
        "    if any(vals >= 0.9999 for vals in token_cos[i]):\n",
        "      copy_paste.append([i,[index for index, vals in enumerate(token_cos[i]) if vals >= 0.9999]])\n",
        "    elif any(0.7 < vals < 0.9999 for vals in token_cos[i]):\n",
        "      struc_token.append([i,[index for index, vals in enumerate(token_cos[i]) if 0.7 < vals < 0.9999]])\n",
        "      token = 1\n",
        "    else:\n",
        "      token = 0\n",
        "\n",
        "    # For plagiarism checking score (other than copy-pasted or 99% similarity)\n",
        "    if token + tf > 1:\n",
        "      plag_tf_token += 1\n",
        "  return plag_tf_token, di, high_sim, copy_paste, struc_token, struc_tf"
      ],
      "metadata": {
        "id": "7MkLbZBtQYc9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to show all lines with high similarity or copy-pasted\n",
        "\n",
        "def show_line(struc_token,struc_tf,text_list,high_sim,copy_paste,j):\n",
        "  # Show all line that has over 99% similarity\n",
        "  if len(high_sim) > 0:\n",
        "    print(\"\\nLine with 99% similarity:\")\n",
        "    for i in range(len(high_sim)):\n",
        "      for k in high_sim[i][1]:\n",
        "        print(f\"\\t {i+1}. {text_list[-1][high_sim[i][0]]} vs {text_list[j][k]}\")\n",
        "\n",
        "  # Show all line that are copy-pasted\n",
        "  if len(copy_paste) > 0:\n",
        "    print(\"\\nCopy-pasted line:\")\n",
        "    for i in range(len(copy_paste)):\n",
        "      for k in copy_paste[i][1]:\n",
        "        print(f\"\\t {i+1}. {text_list[-1][copy_paste[i][0]]} vs {text_list[j][k]}\")\n",
        "\n",
        "  # Show all line that has high similar structure\n",
        "  if len(struc_token) > 0:\n",
        "    print(\"\\nLine with similar structure:\")\n",
        "    for i in range(len(struc_token)):\n",
        "      for k in struc_token[i][1]:\n",
        "        print(f\"\\t {i+1}. {text_list[-1][struc_token[i][0]]} vs {text_list[j][k]}\")\n",
        "\n",
        "  # Show all line that has similarity more than 35%\n",
        "  if len(struc_tf) > 0:\n",
        "    print(\"\\nLine with similarity higher than 35%:\")\n",
        "    for i in range(len(struc_tf)):\n",
        "      for k in struc_tf[i][1]:\n",
        "        print(f\"\\t {i+1}. {text_list[-1][struc_tf[i][0]]} vs {text_list[j][k]}\")"
      ],
      "metadata": {
        "id": "USE8ZnslDtuT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main code of the system, calling all the neccesary functions\n",
        "\n",
        "def main_code(story_data,text_list):\n",
        "  # Flattened the text to smooth out the tokenize and vectorize\n",
        "  flat_text = [line for text in text_list for line in text]\n",
        "\n",
        "  # Calling the tokenizer and vectorizer function\n",
        "  tfid = vecTfid(flat_text,text_list)\n",
        "  token = tokenizing(flat_text,text_list)\n",
        "\n",
        "  # Iterating each text to examine the similarity between text\n",
        "  for j in range(len(token)):\n",
        "    # Skip chapter text of same story\n",
        "    if story_data.iloc[-1,1] != story_data.iloc[j,1]:\n",
        "      plag_tf_token, di,  high_sim, copy_paste, struc_token, struc_tf = count_flag(token[-1], token[j], tfid[-1], tfid[j])\n",
        "      flag_tf = len(high_sim)\n",
        "      flag_token = len(copy_paste)\n",
        "      plag_score = (plag_tf_token+(flag_tf+flag_token)/2)/di\n",
        "      tf_token = len(set([index for index,val in enumerate(struc_token)] + [index for index,val in enumerate(struc_tf)]))\n",
        "\n",
        "      # Plagiarized score are categorized into 3 section\n",
        "      # Those with plagiarism score more than 30% and those with less than 30%\n",
        "      # For plagiarism score with more than 30%, a warning will be displayed, along with the number of copy-pasted and high similarity line\n",
        "      # For plagiarism score with less than 30%, however, has copy-pasted and high similarity line, a warning will be displayed\n",
        "      # For plagiarism score that has less than 30% and no copy-pasted and high similarity line, no warning will be displayed\n",
        "\n",
        "      print(f\"file {story_data.iloc[-1,1]} chapter {story_data.iloc[-1,3]} vs file {story_data.iloc[j,1]} chapter {story_data.iloc[j,3]}\")\n",
        "      if plag_score >= 0.3:\n",
        "        print(f\"\\t!Warning! There are overall {plag_score*100:.2f}% similarity score in both file! Bigger than 30%!\")\n",
        "        if flag_tf > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {flag_tf} line with 99% similarity! About {flag_tf/di*100:.2f}% of the text!\")\n",
        "        if flag_token > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {flag_token} line with 99% similar structure! About {flag_token/di*100:.2f}% of the text!\")\n",
        "        if plag_tf_token > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {tf_token} line with either 70% more similarity or similar structure! About {tf_token/di*100:.2f}% of the text!\")\n",
        "        show_line(struc_token,struc_tf,text_list,high_sim,copy_paste,j)\n",
        "      elif plag_score < 0.3 and (flag_tf > 0 or flag_token > 0):\n",
        "        print(f\"\\t!Warning! Overall there are {plag_score*100:.2f}% similarity score in both file, less than 30%, however:\")\n",
        "        if flag_tf > 0:\n",
        "          print(f\"\\t\\tThere are {flag_tf} line with 99% similarity in both file! About {flag_tf/di*100:.2f}% of the text!\")\n",
        "        if flag_token > 0:\n",
        "          print(f\"\\t\\tThere are {flag_token} line with 99% similar structure in both file! About {flag_token/di*100:.2f}% of the text!\")\n",
        "        if plag_tf_token > 0:\n",
        "          print(f\"\\t\\tAmong which, there are {tf_token} line with either 70% more similarity or similar structure! About {tf_token/di*100:.2f}% of the text!\")\n",
        "        show_line(struc_token,struc_tf,text_list,high_sim,copy_paste,j)\n",
        "      else:\n",
        "          print(f\"\\tSimilarity score is {plag_score*100:.2f}%! Congratulations, you may upload your work :)!\")\n",
        "\n",
        "      print(\"\\n---------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "5NztBYx6SNhM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Plagiarism_Checker(story_data):\n",
        "  text = text_list(story_data)\n",
        "  main_code(story_data,text)"
      ],
      "metadata": {
        "id": "hHdijRDPr8KU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Plagiarism_Checker(request())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi--IDqEtEPW",
        "outputId": "405bbd53-89bf-4f53-eea3-2fad09d355b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file 10 chapter Ikatan vs file 1 chapter Takdir Pertemuan\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "file 10 chapter Ikatan vs file 1 chapter Ikatan\n",
            "\t!Warning! There are overall 91.18% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 9 line with 99% similarity! About 52.94% of the text!\n",
            "\t\tAmong which, there are 10 line with 99% similar structure! About 58.82% of the text!\n",
            "\t\tAmong which, there are 6 line with either 70% more similarity or similar structure! About 35.29% of the text!\n",
            "\n",
            "Line with 99% similarity:\n",
            "\t 1. mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban vs mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban\n",
            "\t 2. sarah menceritakan bahwa ia dan korban pernah berselisih vs sarah menceritakan bahwa ia dan korban pernah berselisih\n",
            "\t 3. korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati vs korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati\n",
            "\t 4. anya dan dimas tidak yakin apakah mereka percaya pada sarah vs anya dan dimas tidak yakin apakah mereka percaya pada sarah\n",
            "\t 5. mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka vs mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka\n",
            "\t 6. sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional vs sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional\n",
            "\t 7. suatu malam anya dan dimas pergi makan malam bersama vs suatu malam anya dan dimas pergi makan malam bersama\n",
            "\t 8. pada akhir malam itu mereka berciuman vs pada akhir malam itu mereka berciuman\n",
            "\t 9. mereka menyadari bahwa mereka telah jatuh cinta vs mereka menyadari bahwa mereka telah jatuh cinta\n",
            "\n",
            "Copy-pasted line:\n",
            "\t 1. mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban vs mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban\n",
            "\t 2. sarah menceritakan bahwa ia dan korban pernah berselisih vs sarah menceritakan bahwa ia dan korban pernah berselisih\n",
            "\t 3. korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati vs korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati\n",
            "\t 4. anya dan dimas tidak yakin apakah mereka percaya pada sarah vs anya dan dimas tidak yakin apakah mereka percaya pada sarah\n",
            "\t 5. mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka vs mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka\n",
            "\t 6. sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional vs sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional\n",
            "\t 7. mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka vs mereka mulai menghabiskan lebih banyak waktu bersama dan mereka mulai merasakan kupu kupu di perut mereka\n",
            "\t 8. suatu malam anya dan dimas pergi makan malam bersama vs suatu malam anya dan dimas pergi makan malam bersama\n",
            "\t 9. pada akhir malam itu mereka berciuman vs pada akhir malam itu mereka berciuman\n",
            "\t 10. mereka menyadari bahwa mereka telah jatuh cinta vs mereka menyadari bahwa mereka telah jatuh cinta\n",
            "\n",
            "Line with similar structure:\n",
            "\t 1. anya dan dimas baru yang mereka temukan di akhir chapter pertama vs anya dan dimas mengikuti petunjuk baru yang mereka temukan di akhir chapter pertama\n",
            "\t 2. sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai vs sarah awalnya menolak untuk berbicara dengan mereka tetapi akhirnya ia setuju untuk diwawancarai\n",
            "\t 3. ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya vs ia mengatakan bahwa ia memiliki motif untuk membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya\n",
            "\t 4. mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain vs mereka mengobrol dan tertawa dan mereka merasa sangat nyaman satu sama lain\n",
            "\t 5. ciuman itu adalah pengalaman hidup anya dan dimas vs ciuman itu adalah pengalaman yang mengubah hidup anya dan dimas\n",
            "\n",
            "Line with similarity higher than 35%:\n",
            "\t 1. anya dan dimas baru yang mereka temukan di akhir chapter pertama vs anya dan dimas mengikuti petunjuk baru yang mereka temukan di akhir chapter pertama\n",
            "\t 2. sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai vs sarah awalnya menolak untuk berbicara dengan mereka tetapi akhirnya ia setuju untuk diwawancarai\n",
            "\t 3. ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya vs ia mengatakan bahwa ia memiliki motif untuk membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya\n",
            "\t 4. mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka vs mereka mulai menghabiskan lebih banyak waktu bersama dan mereka mulai merasakan kupu kupu di perut mereka\n",
            "\t 5. mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain vs mereka mengobrol dan tertawa dan mereka merasa sangat nyaman satu sama lain\n",
            "\t 6. ciuman itu adalah pengalaman hidup anya dan dimas vs ciuman itu adalah pengalaman yang mengubah hidup anya dan dimas\n",
            "\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "file 10 chapter Ikatan vs file 111 chapter Lila yang Tersesat di Paris\n",
            "\t!Warning! Overall there are 11.76% similarity score in both file, less than 30%, however:\n",
            "\t\tThere are 2 line with 99% similarity in both file! About 11.76% of the text!\n",
            "\t\tThere are 2 line with 99% similar structure in both file! About 11.76% of the text!\n",
            "\n",
            "Line with 99% similarity:\n",
            "\t 1. akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia vs akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia\n",
            "\t 2. dia kemudian mengontak kedubes indonesia di paris vs dia kemudian mengontak kedubes indonesia di paris\n",
            "\n",
            "Copy-pasted line:\n",
            "\t 1. akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia vs akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia\n",
            "\t 2. dia kemudian mengontak kedubes indonesia di paris vs dia kemudian mengontak kedubes indonesia di paris\n",
            "\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "file 10 chapter Ikatan vs file 16 chapter Persahabatan yang Tak Terlupakan\n",
            "\tSimilarity score is 0.00%! Congratulations, you may upload your work :)!\n",
            "\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "file 10 chapter Ikatan vs file 5 chapter Ikatan\n",
            "\t!Warning! There are overall 100.00% similarity score in both file! Bigger than 30%!\n",
            "\t\tAmong which, there are 17 line with 99% similarity! About 100.00% of the text!\n",
            "\t\tAmong which, there are 17 line with 99% similar structure! About 100.00% of the text!\n",
            "\n",
            "Line with 99% similarity:\n",
            "\t 1. anya dan dimas baru yang mereka temukan di akhir chapter pertama vs anya dan dimas baru yang mereka temukan di akhir chapter pertama\n",
            "\t 2. mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban vs mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban\n",
            "\t 3. sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai vs sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai\n",
            "\t 4. sarah menceritakan bahwa ia dan korban pernah berselisih vs sarah menceritakan bahwa ia dan korban pernah berselisih\n",
            "\t 5. korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati vs korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati\n",
            "\t 6. ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya vs ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya\n",
            "\t 7. anya dan dimas tidak yakin apakah mereka percaya pada sarah vs anya dan dimas tidak yakin apakah mereka percaya pada sarah\n",
            "\t 8. mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka vs mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka\n",
            "\t 9. sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional vs sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional\n",
            "\t 10. mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka vs mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka\n",
            "\t 11. suatu malam anya dan dimas pergi makan malam bersama vs suatu malam anya dan dimas pergi makan malam bersama\n",
            "\t 12. mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain vs mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain\n",
            "\t 13. pada akhir malam itu mereka berciuman vs pada akhir malam itu mereka berciuman\n",
            "\t 14. ciuman itu adalah pengalaman hidup anya dan dimas vs ciuman itu adalah pengalaman hidup anya dan dimas\n",
            "\t 15. mereka menyadari bahwa mereka telah jatuh cinta vs mereka menyadari bahwa mereka telah jatuh cinta\n",
            "\t 16. akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia vs akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia\n",
            "\t 17. dia kemudian mengontak kedubes indonesia di paris vs dia kemudian mengontak kedubes indonesia di paris\n",
            "\n",
            "Copy-pasted line:\n",
            "\t 1. anya dan dimas baru yang mereka temukan di akhir chapter pertama vs anya dan dimas baru yang mereka temukan di akhir chapter pertama\n",
            "\t 2. mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban vs mereka melacak seorang wanita bernama sarah yang merupakan mantan kekasih korban\n",
            "\t 3. sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai vs sarah awalnya menolak tetapi akhirnya ia setuju untuk diwawancarai\n",
            "\t 4. sarah menceritakan bahwa ia dan korban pernah berselisih vs sarah menceritakan bahwa ia dan korban pernah berselisih\n",
            "\t 5. korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati vs korban telah berselingkuh dengan wanita lain dan sarah merasa sangat sakit hati\n",
            "\t 6. ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya vs ia mengatakan bahwa ia membunuh korban tetapi ia bersikeras bahwa ia tidak melakukannya\n",
            "\t 7. anya dan dimas tidak yakin apakah mereka percaya pada sarah vs anya dan dimas tidak yakin apakah mereka percaya pada sarah\n",
            "\t 8. mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka vs mereka masih mencari bukti yang lebih kuat untuk mendukung tuduhan mereka\n",
            "\t 9. sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional vs sementara itu anya dan dimas mulai menyadari bahwa mereka memiliki ketertarikan satu sama lain yang lebih dari sekadar profesional\n",
            "\t 10. mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka vs mereka mulai menghabiskan lebih banyak waktu bersama dan merasakan kupu kupu di perut mereka\n",
            "\t 11. suatu malam anya dan dimas pergi makan malam bersama vs suatu malam anya dan dimas pergi makan malam bersama\n",
            "\t 12. mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain vs mereka mengobrol dan tertawa dan mereka sangat nyaman satu sama lain\n",
            "\t 13. pada akhir malam itu mereka berciuman vs pada akhir malam itu mereka berciuman\n",
            "\t 14. ciuman itu adalah pengalaman hidup anya dan dimas vs ciuman itu adalah pengalaman hidup anya dan dimas\n",
            "\t 15. mereka menyadari bahwa mereka telah jatuh cinta vs mereka menyadari bahwa mereka telah jatuh cinta\n",
            "\t 16. akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia vs akhirnya polisi wanita itu mengetahui bahwa lila adalah orang indonesia\n",
            "\t 17. dia kemudian mengontak kedubes indonesia di paris vs dia kemudian mengontak kedubes indonesia di paris\n",
            "\n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "er86YNDEtKDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}